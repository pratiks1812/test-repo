{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd \n",
    "from config import AGES, median\n",
    "from functions import get_age, build_query, parse_float\n",
    "import os \n",
    "import itertools\n",
    "import numpy as np \n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function definition\n",
    "# --------------------\n",
    "\n",
    "# function to set variable values\n",
    "def create_new_variable(row, frame):\n",
    "    subset = frame[frame.DATAYEAR == row.year]\n",
    "    res = subset.query(row['conditions'])\n",
    "    if not res.empty:\n",
    "        frame.loc[res.index, row.variable] = row.code \n",
    "    \n",
    "\n",
    "      \n",
    "# function to count how many new_variables should be created\n",
    "def count_new_variable(row, frame):\n",
    "    subset = frame[frame.DATAYEAR == row.year]\n",
    "    index = pd.MultiIndex.from_tuples([(row.variable, row.code)], names=['variable', 'code'])\n",
    "    columns = pd.MultiIndex.from_tuples([('Frequency', row.year), ('Weighted Frequency', row.year)])\n",
    "    \n",
    "    # query results\n",
    "    qres = subset.query(row['conditions'])\n",
    "    \n",
    "    frequency = 0 if qres.empty else qres.shape[0]\n",
    "    weighted_frequency = 0 if qres.empty else qres.FINALWT.sum()\n",
    "    \n",
    "    # output results\n",
    "    res = pd.DataFrame(index=index, columns=columns, data = [[frequency, weighted_frequency]])\n",
    "    return res \n",
    "        \n",
    "# explode loop function\n",
    "def explode_loop(frame):\n",
    "    cols = list(frame)\n",
    "    colvals = [frame[col].dropna().drop_duplicates().unique() if col != 'DATAYEAR' else frame[col].dropna().unique() for col in list(frame)]\n",
    "    res = pd.DataFrame([dict(list(zip(cols, combo))) for combo in itertools.product(*colvals)])\n",
    "    \n",
    "    return res\n",
    "\n",
    "# from a given dataframe, find all the rows in which a given column is not na \n",
    "def get_rows_notna(frame, column):\n",
    "    return frame[frame[column].notna()].index\n",
    "\n",
    "def update_frame_with_loop_row(frame, column, row):\n",
    "    # get all the indicies where the given column is not na \n",
    "    inds = get_rows_notna(frame, column)\n",
    "    \n",
    "    \n",
    "    frame_update = pd.concat([row.to_frame().T[[column]]]*len(inds))\n",
    "    frame_update.index = inds \n",
    "\n",
    "    frame.update(frame_update)\n",
    "    \n",
    "def grab_variable_labels(varname):\n",
    "    labels = nvdf[nvdf.variable == varname].sort_values('code')[['label', 'code']].drop_duplicates().values\n",
    "  \n",
    "    labels =  [tuple(x) for x in labels] + [('', 'Blank')]\n",
    "    return pd.DataFrame(index = pd.MultiIndex.from_tuples(labels, names=['label', 'code']),\n",
    "                        columns = pd.MultiIndex.from_tuples([('Frequency', -1), ('Weighted Frequency', -1)])) \n",
    "    \n",
    "def get_count(frame, column):\n",
    "    try:\n",
    "        valcount = frame[column].value_counts(dropna=False).sort_index()\n",
    "    except TypeError as e :\n",
    "        print(column)\n",
    "        raise e\n",
    "    valcount = valcount.to_frame()\n",
    "    \n",
    "    valcount['FINALWT'] = valcount.apply(lambda row: frame[frame[column].isin([row.name])].FINALWT.sum(), axis=1)\n",
    "    valcount.index = valcount.index.fillna('Blank')\n",
    "    valcount.index.names = ['code']\n",
    "    \n",
    "    return valcount\n",
    "    \n",
    "def count_present_new_variables(column, labels, frame):\n",
    "    res = frame.groupby('DATAYEAR').apply(get_count, column)\n",
    "    res.columns = ['Frequency', 'Weighted Frequency']\n",
    "    res = res.pivot_table(columns=['DATAYEAR'], index=['code'])\n",
    "    minyear = frame.DATAYEAR.min()\n",
    "    maxyear = frame.DATAYEAR.max()\n",
    "    # the labels column will introduce -1 values to the `weighted and unweighted frequncy` columns\n",
    "    res = pd.merge(labels, res, left_index=True, right_index=True, how='outer').sort_index(axis=1)\n",
    "    # removing them using index splicing\n",
    "    return res.loc[:, pd.IndexSlice[:, minyear:maxyear]]\n",
    "\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_from_query(condition, frame):\n",
    "    if condition and (condition != 'None'):\n",
    "        res = frame.query(condition)\n",
    "        \n",
    "        s = pd.Series(dtype=float)\n",
    "        if not res.empty:\n",
    "            s['Frequency'] = res.shape[0]\n",
    "            s['Weighted Frequency'] = res['FINALWT'].sum()\n",
    "        else:\n",
    "            s['Frequency'] = 0\n",
    "            s['Weighted Frequency'] = 0 \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all of this into a function which you can use for loop.apply(lambda row: ...)\n",
    "# params so far: row, nvdf (new_variable_frame), analysis_df\n",
    "\n",
    "# update the coresponding columns new dataframe tables. \n",
    "\n",
    "def create_variables_and_perform_analysis(row, df, nvdf, dfs):\n",
    "    # after you have updated the column values, use the the loop's row to filter out the dataframe?\n",
    "    filter_str = build_query(**row.to_dict())\n",
    "    if not nvdf.empty:\n",
    "        df.drop(columns=new_variables, inplace=True, errors='ignore')\n",
    "    df1 = df.query(filter_str)\n",
    "\n",
    "\n",
    "    # create new variables if it's not empty\n",
    "    if not nvdf.empty:\n",
    "        df1[new_variables] = np.nan\n",
    "        # filter the new variables to appropriate year \n",
    "        # create year_new_variable_df == ynvdf\n",
    "        ynvdf = nvdf[nvdf.year == row.DATAYEAR]\n",
    "\n",
    "        # if the year is not present in the new variable df (nvdf) then skip this year \n",
    "        # if ynvdf.empty:\n",
    "        #     return \n",
    "\n",
    "        for rowname in row.index:\n",
    "            if rowname in ynvdf:\n",
    "                notnaindex = ynvdf[ynvdf[rowname].notna()].index\n",
    "                if len(notnaindex) == 0:\n",
    "                    if rowname == '_AGEG5YR':\n",
    "                        notnaindex = ynvdf.index\n",
    "                    else:\n",
    "                        continue\n",
    "                naupdate = pd.concat([row.to_frame().T]*len(notnaindex)).reset_index(drop=True)\n",
    "                \n",
    "                # ynvdf subset == ts\n",
    "                ynvdf.loc[notnaindex, rowname] = row.loc[rowname]\n",
    "        \n",
    "        ynvdf =  ynvdf.sort_values(['year', 'order', 'variable'])\n",
    "        # create conditions based on the updates\n",
    "        ynvdf['conditions'] = ynvdf.apply(lambda row: build_query(**row[variable_cols].dropna().to_dict()),axis=1)\n",
    "        \n",
    "        # create new variables based on filtered year data\n",
    "        ynvdf.apply(lambda row: create_new_variable(row, df1), axis=1)\n",
    "        \n",
    "        # if new variable breakdown requested. \n",
    "\n",
    "    # lst=[{k:v} for k,v in dfs.items()]\n",
    "    # lst=lst[0]    \n",
    "    # apply analysis\n",
    "    # for name, table in lst.items():\n",
    "        \n",
    "    for name, table in dfs.items():\n",
    "        tb = table.copy(deep=True)\n",
    "        output_selector = str(row.drop('DATAYEAR', errors='ignore').values.tolist() + [name])\n",
    "            \n",
    "        for cols in tb:\n",
    "            nona =tb[tb[cols].notna()] \n",
    "            match = nona[nona[cols].astype(str).str.contains(ptn1)]\n",
    "            if not match.empty:\n",
    "                nomatch = match[~match[cols].str.contains(str(row.DATAYEAR))]\n",
    "                match = match.drop(index = nomatch.index)\n",
    "                nomatch[cols] = nomatch[cols].replace(ptn1, '', regex=True)\n",
    "                match[cols] = match[cols].apply(replace_w_match, pattern=ptn3)\n",
    "                match = pd.concat([match, nomatch])\n",
    "                tb.loc[match.index] = match\n",
    "     \n",
    "        for rowname in row.index:\n",
    "            if rowname in tb:\n",
    "                notnaindex = tb[tb[rowname].notna()].index\n",
    "                if len(notnaindex) == 0:\n",
    "                    if rowname == '_AGEG5YR':\n",
    "                        notnaindex = tb.index\n",
    "                    else:\n",
    "                        continue\n",
    "                naupdate = pd.concat([row.to_frame().T]*len(notnaindex)).reset_index(drop=True)\n",
    "                \n",
    "                # table subset == ts\n",
    "                tb.loc[notnaindex, rowname] = row.loc[rowname]\n",
    "                \n",
    "        if not output_selector in outputs:\n",
    "            outputs[output_selector] = tb.drop(columns='conditions', errors='ignore').copy(deep=True)\n",
    "            outputs[output_selector] = pd.DataFrame()\n",
    "\n",
    "        varcols = [col for col in list(tb) if col.upper() == col]\n",
    "        tb['conditions'] = tb.apply(lambda row: build_query(**row[varcols].dropna().to_dict()),axis=1)\n",
    "        df3 = tb.apply(lambda x: count_from_query(x.conditions, df1), axis=1)\n",
    "        df3.columns = pd.MultiIndex.from_tuples([('Frequency', row.DATAYEAR), ( 'Weighted Frequency', row.DATAYEAR)])\n",
    "        \n",
    "        # print(df3.columns)\n",
    "        # print(df3.shape)\n",
    "        # print(outputs[output_selector].shape)\n",
    "        # print(outputs[output_selector].columns)\n",
    "\n",
    "        outputs[output_selector] = pd.merge(outputs[output_selector], df3, left_index=True, right_index=True, how='outer').sort_index(axis=1)\n",
    "         \n",
    "\n",
    "     # create new variables based on filtered year data\n",
    "    if not nvdf.empty:\n",
    "        df1.drop(columns=new_variables, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_create_variables_and_perform_analysis_create_new_variables(row, df, nvdf):\n",
    "    filter_str = build_query(**row.to_dict())\n",
    "    if not nvdf.empty:\n",
    "        df.drop(columns=new_variables, inplace=True, errors='ignore')\n",
    "    df1 = df.query(filter_str)\n",
    "\n",
    "    if not nvdf.empty:\n",
    "        df1[new_variables] = np.nan\n",
    "        # filter the nvew variables to appropriate year \n",
    "        # create year_new_variable_df == nvdf\n",
    "        update = pd.concat([row.to_frame().T]*len(nvdf)).reset_index(drop=True)\n",
    "        nvdf.update(update)\n",
    "        if '_AGEG5YR' in row:\n",
    "            if row._AGEG5YR in [np.nan]:\n",
    "                nvdf['_AGEG5YR'] = row._AGEG5YR\n",
    "  \n",
    "        \n",
    "        \n",
    "        # nvdf =  nvdf.sort_values(['year', 'order', 'variable'])\n",
    "        # create conditions based on the updates\n",
    "        nvdf['conditions'] = nvdf.apply(lambda row: build_query(**row[variable_cols].dropna().to_dict()),axis=1)\n",
    "        \n",
    "        # create new variables based on filtered year data\n",
    "        df1.drop(columns=new_variables, inplace=True, errors='ignore')\n",
    "        df1[new_variables] = np.nan\n",
    "        nvdf.apply(lambda row: create_new_variable(row, df1), axis=1)\n",
    "        \n",
    "\n",
    "    return pd.concat([vnames, vnames.apply(lambda x: count_present_new_variables(x.varname, x.labels, df1), axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS\n",
    "ptn1 = '\\(.* in \\d{2,4}\\)'\n",
    "ptn2 = '(?<=\\s)\\d{2,4}'\n",
    "ptn3 = '(?<=\\().*(?=\\sin)'\n",
    "def replace_w_match(string, pattern):\n",
    "    res = re.search(pattern, string).group()\n",
    "    if '**' in string :\n",
    "        res += ' **' \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 1: open files \n",
    "# --------------------\n",
    "\n",
    "xl = pd.ExcelFile(r'C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\Input folder - zip\\settings.xlsx')\n",
    "settings = xl.parse('settings', index_col='setting')\n",
    "loop = None if settings.loc['LOOP','value'] in [None, False] else xl.parse('loop', na_filter=False).replace('', np.nan)\n",
    "xl.close()\n",
    "\n",
    "# analysis excel sheet\n",
    "axl = pd.ExcelFile(settings.loc['ANALYSIS FILE LOCATION', 'value'])\n",
    "\n",
    "pages = settings.loc['ANALYSIS PAGE NAMES', 'value']\n",
    "if isinstance(pages, str):\n",
    "    pages = pages.split(';')\n",
    "elif isinstance(pages, (int,float)):\n",
    "    pages = str(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= READING VARIABLE SET UP FILE =============\n",
      "========= PARSING DATA ANALYSIS PAGES INTO LOCAL MEMORY =============\n"
     ]
    }
   ],
   "source": [
    "## step 1.1 : read data required files\n",
    "# --------------------\n",
    "\n",
    "# print('========= READING DATA =============')\n",
    "df = pd.read_csv(settings.loc['DATA LOCATION','value'])\n",
    "# df.rename(columns={'VCLTEST':'VCLNTEST'},inplace=True)\n",
    "\n",
    "print('========= READING VARIABLE SET UP FILE =============')\n",
    "# new variable dataframe will be empty if new variables == false and you'd open it if it isn't\n",
    "nvdf = pd.DataFrame() if not settings.loc['NEW VARIABLES', 'value'] else pd.read_excel(settings.loc['NEW VARIABLE LOCATION', 'value'], na_filter=False).replace('', np.nan)\n",
    "outpath = settings.loc['OUTPUT PATH', 'value']\n",
    "\n",
    "print('========= PARSING DATA ANALYSIS PAGES INTO LOCAL MEMORY =============')\n",
    "# analysis tables\n",
    "dfs = {page.strip(): axl.parse(str(page.strip()), skiprows=(settings.loc['ANALYSIS START ROW', 'value'])-1) for page in pages}\n",
    "axl.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VARIABLES : \"True\"\n",
      "NEW VARIABLE LOCATION : \"C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\Input folder - zip\\NEW VARIABLES\\colorectal_analysis_new_variables_definition_file.xlsx\"\n",
      "ANALYSIS PAGE NAMES : \"1;2;3;4\"\n",
      "ANALYSIS START ROW : \"4\"\n",
      "ANALYSIS FILE LOCATION : \"C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\Input folder - zip\\Colorectal data format UPDATED.xlsx\"\n",
      "LOOP : \"True\"\n",
      "OUTPUT PATH : \"C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\output_test\\new\\Colorectal Data Analysis-{0}.xlsx\"\n",
      "DATA LOCATION : \"C:\\Users\\AnkitB\\OneDrive - crisil.com\\backup\\Bernstein\\output_test\\new\\BRFSS_COLORECTAL.csv\"\n"
     ]
    }
   ],
   "source": [
    "for ind, col in settings.iterrows():\n",
    "    print(ind,':', f'\"{col.values[0]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT BELOW IF YOU NEED TO REDOWNLOAD THE NEW VARIABLE DATAFRAME\n",
    "# nvdf = pd.DataFrame() if not settings.loc['NEW VARIABLES', 'value'] else pd.read_excel(settings.loc['NEW VARIABLE LOCATION', 'value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEPS to follow \n",
    "-------------\n",
    " step 1:\n",
    " open settings file\n",
    "\n",
    " step 2: \n",
    " check if you are creating new variables for this analysis \n",
    " if yes, open the location of the settings file and then work on creating new variables\n",
    "\n",
    " step 3:\n",
    " read the line which tells you which pages you're performing your analysis on (names separated by square brackets\n",
    ")\n",
    " read the tables in each page you're applying your queries to.drop any na rows\n",
    " get the list of years you will be applying this to. \n",
    " get list of looping variables you will be applying the same function to\n",
    "\n",
    " step 4:\n",
    " filter to DATAYEAR variable given in the list of years \n",
    " create conditions for each subset by looping through the rows\n",
    " apply conditions and count outputs by creating multi-index for year and frequency / weighted frequency\n",
    "\n",
    " step 5:\n",
    " prepare way to write to back to openpyxl in a specific order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = explode_loop(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<45', '45,50', '50,75', '75,80', '>80', '#NA'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l._AGEG5YR.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: check new variables, create if true\n",
    "\n",
    "if not nvdf.empty:\n",
    "    # filtering out all the year variable for now\n",
    "#     nvdf = nvdf[~nvdf.variable.str.startswith('YEAR')].sort_values(['year', 'order'])\n",
    "    \n",
    "    # reindex columns to include new variables being crated\n",
    "    df = df.reindex(list(df)+list(nvdf.variable.unique()),  axis=1)\n",
    "    variable_cols = [col for col in list(nvdf) if col == col.upper()]\n",
    "    \n",
    "    ndf = nvdf.copy(deep=True)\n",
    "    ndf = ndf.sort_values(by=['year', 'order'])\n",
    "    \n",
    "    new_variables = nvdf.variable.unique()\n",
    "    \n",
    "    vnames = pd.DataFrame(data=new_variables, columns=['varname'])\n",
    "    vnames['labels'] = vnames.varname.apply(grab_variable_labels)\n",
    "# if you have a loop utilise it by exploding it then apply the changes to each row\n",
    "# if not loop.empty:\n",
    "#     loop = explode_loop(loop)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,row in l.sort_values('DATAYEAR').iterrows():\n",
    "    filter_str = build_query(**row.to_dict())\n",
    "    if not nvdf.empty:\n",
    "        df.drop(columns=new_variables, inplace=True, errors='ignore')\n",
    "    df1 = df.query(filter_str)\n",
    "\n",
    "    df1[new_variables] = np.nan\n",
    "    # filter the new variables to appropriate year \n",
    "    # create year_new_variable_df == ynvdf\n",
    "    ynvdf = nvdf[nvdf.year == row.DATAYEAR]   \n",
    "\n",
    "    # if the year is not present in the new variable df (nvdf) then skip this \n",
    "    # if ynvdf.empty:\n",
    "    #     print('Data year', row.DATAYEAR, 'not found in new variable dataframe')\n",
    "    #     continue\n",
    "\n",
    "    for rowname in row.index:\n",
    "        if rowname in ynvdf:\n",
    "            notnaindex = ynvdf[ynvdf[rowname].notna()].index\n",
    "            if len(notnaindex) == 0:\n",
    "                if rowname == '_AGEG5YR':\n",
    "                    notnaindex = ynvdf.index\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            naupdate = pd.concat([row.to_frame().T]*len(notnaindex)).reset_index(drop=True)\n",
    "     \n",
    "\n",
    "            # ynvdf subset == ts\n",
    "            ynvdf.loc[notnaindex, rowname] = row.loc[rowname]\n",
    "            \n",
    "    ynvdf = ynvdf.sort_values(['year', 'order', 'variable'])\n",
    "    # create conditions based on the updates\n",
    "    ynvdf['conditions'] = ynvdf.apply(lambda row: build_query(**row[variable_cols].dropna().to_dict()),axis=1)\n",
    "\n",
    "    # create new variables based on filtered year data\n",
    "    ynvdf.apply(lambda row: create_new_variable(row, df1), axis=1)\n",
    "    \n",
    "    if \"#NA\" in row.values:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022]\n"
     ]
    }
   ],
   "source": [
    "print(df.DATAYEAR.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = {}\n",
    "l.apply(create_variables_and_perform_analysis, args=(df, nvdf, dfs), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopcols = list(loop.drop(columns='DATAYEAR'))\n",
    "for i in outputs:\n",
    "    group = eval(i)\n",
    "    name = group[-1]\n",
    "    d = dfs[name].drop(columns='conditions', errors='ignore')\n",
    "    \n",
    "    for ind, rowname in enumerate(loopcols):       \n",
    "        if rowname in d:\n",
    "            notnaindex = d[d[rowname].notna()].index\n",
    "            if len(notnaindex) == 0:\n",
    "                if rowname == '_AGEG5YR':\n",
    "                    notnaindex = d.index\n",
    "                else:\n",
    "                    continue \n",
    "            d.loc[notnaindex, rowname] = group[ind]\n",
    "        else:\n",
    "            print(f'{rowname} not in d\\n', d.columns)\n",
    "    d.columns = pd.MultiIndex.from_tuples(tuple([('', x) for x in list(d)]))\n",
    "    \n",
    "    c = outputs[i]\n",
    "    c.columns = pd.MultiIndex.from_tuples(tuple([(str(x) for x in x) for x in list(c)]))\n",
    "    \n",
    "    outputs[i] = pd.merge(d, c, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_groups = {}\n",
    "for page in dfs:\n",
    "    if not page in data_groups:\n",
    "        res = [outputs[x] for x in outputs if eval(x)[-1] == page]\n",
    "        out = pd.DataFrame()\n",
    "        for x in res:\n",
    "            out = pd.concat([out, x, pd.DataFrame()])\n",
    "        out.reset_index(drop=True, inplace=True)\n",
    "        data_groups[page] = out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(settings.loc['OUTPUT PATH', 'value'].format(str(today.date())))\n",
    "for k,v in data_groups.items():\n",
    "    v.to_excel(writer, sheet_name=k)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.loc['OUTPUT PATH', 'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = dfs['1'][list(dfs['1'])[3:]].dropna(how='all').apply(lambda x: x.dropna().to_dict(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries.apply(lambda x: build_query(**x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_query(**{'EVERANYSCREEN': 1.0, 'UPTODATEANY': 1.0, '_AGEG5YR': '45-49'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_AGEGROUP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31368\\2314523592.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;31m# testing for age group <45,50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AGEGROUP\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5898\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5899\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5900\u001b[0m         ):\n\u001b[0;32m   5901\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5902\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_AGEGROUP'"
     ]
    }
   ],
   "source": [
    "# testing for age group <45,50 \n",
    "\n",
    "df[df._AGEGROUP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
